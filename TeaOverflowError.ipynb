{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "TeaOverflowError\n",
    "</font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazirmatn\" color=\"#0099cc\">\n",
    "مقدمه و صورت مسئله\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazirmatn\" size=3>\n",
    "\n",
    "مدیرعامل شرکت معروف HonkHonk برای طراحی یک سیستم قیمت‌گذار پویا برای جایگزینی سیستم قبلی شرکت نیاز دارد. سیستم جدید باید تا حد امکان شبیه به سیستم قبلی باشد. به همین منظور مدیرعامل مجموعه داده‌ای برای یافتن نحوه قیمت‌گذاری سیستم قبلی در اختیار شما قرار داده است.\n",
    "\n",
    "با استفاده از این مجموعه داده و تکنیک‌های برنامه‌نویسی، هوش مصنوعی، یادگیری ماشین و جمع‌آوری داده‌های کمکی مورد نیاز بهترین سیستم ممکن برای جایگزینی این سیستم طراحی کنید.\n",
    "\n",
    "\n",
    "\n",
    "</font>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معرفی مجموعه داده\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در فایل اولیه‌ی این سوال، یک پوشه با نام <code>Data</code> وجود دارد که شامل ۲ فایل <code>train.csv</code> و <code>test.csv</code> است. این ۲ فایل شامل داده‌های از قیمت‌گذاری سیستم مرحوم برای سفرهای ثبت‌شده در سال ۲۰۱۶ در شهر نیویورک است.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "هرستون این مجموعه‌داده با توجه به ماهیت خود می‌تواند مقادیر عددی یا رشته‌ای داشته باشد. برای آشنایی بیشتر با مجموعه‌داده، پیشنهاد می‌شود هرستون به صورت جداگانه بررسی شود.</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معرفی مجموعه داده آموزشی (train.csv)\n",
    "</font>\n",
    "</h4>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "\n",
    "فایل <code>train.csv</code> دارای  `11` ستون می‌باشد که ستون‌های آن به شرح زیر می‌باشد:\n",
    "\n",
    "<center>\n",
    "<div dir=rtl style=\"direction: rtl;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    \n",
    "\n",
    "| نام ستون | توضیحات                                |\n",
    "| :----------------------: | :-----------------------------: |\n",
    "| `id`                    | یک شناسه منحصر به فرد برای هر سفر.                     |\n",
    "| `pickup_datetime`       | تاریخ و زمان شروع سفر.                               |\n",
    "| `dropoff_datetime`      | تاریخ و زمان پایان سفر.                               |\n",
    "| `passenger_count`       | تعداد مسافران در خودرو.                              |\n",
    "| `pickup_longitude`      | طول جغرافیایی مکان مبدا سفر.                         |\n",
    "| `pickup_latitude`       | عرض جغرافیایی مکان مبدا سفر.                         |\n",
    "| `dropoff_longitude`     | طول جغرافیایی مکان مقصد سفر.                         |\n",
    "| `dropoff_latitude`      | عرض جغرافیایی مکان مقصد سفر.                         |\n",
    "| `store_and_fwd_flag`    | نشان‌دهنده اینکه آیا اطلاعات سفر قبل از ارسال به سرور، در حافظه خودرو ذخیره شده بود یا خیر. (Y/N) |\n",
    "| `trip_duration`         | کل مدت زمان سفر به ثانیه.                           |\n",
    "| `total_price`           |  قیمت نهایی کل سفر به دلار  |\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معرفی مجموعه داده آزمون (test.csv)\n",
    "</font>\n",
    "</h4>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "\n",
    "فایل <code>test.csv</code> ستون‌هایی مشابه با فایل `test.csv` دارد، با این تفاوت که فاقد ستون `total_price` می‌باشد.\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "در این قسمت کتاب‌خانه‌ها و ابزار مورد نیاز خود را <code>import</code> کنید و فایل داده‌ها را که در پوشه‌ی <code>Data</code> و که با نام‌های <code>train.csv</code> و <code>test.csv</code> ذخیره‌شده‌اند را بخوانید و وارد محیط کار خود کنید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# --- header: imports, seed, config\n",
    "import os, gc, warnings, zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# LightGBM, CatBoost, XGBoost\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "SEEDS = [42, 11, 2023]\n",
    "N_SPLITS = 5        # Time-based CV folds\n",
    "N_CLUSTERS = 50     # KMeans clusters for location\n",
    "np.random.seed(42)\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "USE_GPU = False     # Set to True if GPU available\n",
    "\n",
    "# --- data paths\n",
    "TRAIN_PATH = \"../Data/train.csv\" if os.path.exists(\"../Data/train.csv\") else \"./Data/train.csv\"\n",
    "TEST_PATH = \"../Data/test.csv\" if os.path.exists(\"../Data/test.csv\") else \"./Data/test.csv\"\n",
    "SUB_PATH = \"submission.csv\"\n",
    "RESULT_ZIP = \"result.zip\"\n",
    "MODEL_DIR = \"models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "پیش‌پردازش و مهندسی ویژگی\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "        در این سوال شما می‌توانید از هر تکنیک پیش‌پردازش/مهندسی ویژگی دلخواهتان، استفاده کنید.\n",
    "    <br>\n",
    "    تکنیک‌هایی که استفاده می‌کنید به شکل مستقیم مورد ارزیابی توسط سامانه داوری قرار <b>نمی‌گیرند.</b> بلکه همه آن‌ها در دقت مدل شما تاثیر خواهند گذاشت؛ بنابراین هر چه پیش‌پردازش/مهندسی ویژگی بهتری انجام دهید تا دقت مدل بهبود پیدا کند، امتیاز بیشتری از این سوال کسب خواهید کرد.\n",
    "    در این قسمت شما می‌توانید بخشی از داده‌ی موجود را برای اعتبارسنجی در نظر بگیرید.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- helper: Haversine distance, Manhattan, Bearing\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  # km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "def manhattan(lat1, lon1, lat2, lon2):\n",
    "    # Manhattan approximation on lat/lon grid\n",
    "    x = haversine(lat1, lon1, lat1, lon2)\n",
    "    y = haversine(lat1, lon1, lat2, lon1)\n",
    "    return x + y\n",
    "\n",
    "def bearing(lat1, lon1, lat2, lon2):\n",
    "    # Initial bearing in degrees\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlon = lon2 - lon1\n",
    "    x = np.sin(dlon) * np.cos(lat2)\n",
    "    y = np.cos(lat1)*np.sin(lat2) - np.sin(lat1)*np.cos(lat2)*np.cos(dlon)\n",
    "    return np.degrees(np.arctan2(x, y)) % 360\n",
    "\n",
    "def cyclic_encode(arr, max_val):\n",
    "    arr = np.array(arr)\n",
    "    return np.sin(2 * np.pi * arr / max_val), np.cos(2 * np.pi * arr / max_val)\n",
    "\n",
    "def is_holiday(dt):\n",
    "    # US EN public holidays 2016 (approx)\n",
    "    holidays = {\n",
    "        '2016-01-01', '2016-01-18', '2016-02-15', '2016-05-30',\n",
    "        '2016-07-04', '2016-09-05', '2016-10-10', '2016-11-11',\n",
    "        '2016-11-24', '2016-12-26'\n",
    "    }\n",
    "    return dt.strftime('%Y-%m-%d') in holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1255094, 11), Test shape: (203550, 10)\n",
      "                id      pickup_datetime     dropoff_datetime  passenger_count  \\\n",
      "0        id3702621  2016-06-07 08:57:04  2016-06-07 09:05:47                1   \n",
      "1        id2583306  2016-02-12 16:51:40  2016-02-12 16:56:23                2   \n",
      "1255092  id3849280  2016-01-22 02:50:31  2016-01-22 03:08:24                1   \n",
      "1255093  id1909304  2016-05-03 18:16:00  2016-05-03 18:36:48                2   \n",
      "\n",
      "         pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
      "0              -73.951935        40.777950         -73.960419   \n",
      "1              -73.974289        40.742496         -73.971741   \n",
      "1255092        -73.991287        40.755386         -73.948639   \n",
      "1255093        -73.980980        40.737656         -73.984016   \n",
      "\n",
      "         dropoff_latitude store_and_fwd_flag  trip_duration  total_price  \n",
      "0               40.766243                  N            523    14.045928  \n",
      "1               40.749649                  N            283    15.291179  \n",
      "1255092         40.808270                  N           1073    26.751263  \n",
      "1255093         40.757229                  N           1248    27.261338  \n",
      "count    1.255094e+06\n",
      "mean     2.243004e+01\n",
      "std      4.811024e+01\n",
      "min      4.508333e+00\n",
      "25%      1.218460e+01\n",
      "50%      1.697057e+01\n",
      "75%      2.543286e+01\n",
      "max      2.941182e+04\n",
      "Name: total_price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def rushhour(hour):\n",
    "    # NYC typical rush hour: 7-9, 16-19\n",
    "    return int(hour in range(7,10) or hour in range(16,20))\n",
    "\n",
    "def daypart(hour):\n",
    "    # 0-5:night, 6-11:morning, 12-17:afternoon, 18-23:evening\n",
    "    if hour < 6: return 0\n",
    "    elif hour < 12: return 1\n",
    "    elif hour < 18: return 2\n",
    "    else: return 3\n",
    "\n",
    "# --- load train/test, minimal EDA\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n",
    "print(pd.concat([train.head(2), train.tail(2)]))\n",
    "print(train['total_price'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering train...\n"
     ]
    }
   ],
   "source": [
    "# --- feature engineering\n",
    "def build_features(df, fit_kmeans=None):\n",
    "    # Parse datetime, time features\n",
    "    pickup_dt = pd.to_datetime(df['pickup_datetime'])\n",
    "    dropoff_dt = pd.to_datetime(df['dropoff_datetime'])\n",
    "    df['pickup_year'] = pickup_dt.dt.year\n",
    "    df['pickup_month'] = pickup_dt.dt.month\n",
    "    df['pickup_day'] = pickup_dt.dt.day\n",
    "    df['pickup_hour'] = pickup_dt.dt.hour\n",
    "    df['pickup_minute'] = pickup_dt.dt.minute\n",
    "    df['pickup_weekday'] = pickup_dt.dt.weekday\n",
    "    df['is_weekend'] = df['pickup_weekday'].isin([5,6]).astype(int)\n",
    "    df['pickup_hour_sin'], df['pickup_hour_cos'] = cyclic_encode(df['pickup_hour'], 24)\n",
    "    df['pickup_minute_sin'], df['pickup_minute_cos'] = cyclic_encode(df['pickup_minute'], 60)\n",
    "    df['rushhour'] = df['pickup_hour'].apply(rushhour)\n",
    "    df['daypart'] = df['pickup_hour'].apply(daypart)\n",
    "    df['is_night'] = ((df['pickup_hour'] < 6) | (df['pickup_hour'] >= 23)).astype(int)\n",
    "    df['is_holiday'] = pickup_dt.apply(is_holiday).astype(int)\n",
    "    # Trip features\n",
    "    df['haversine'] = haversine(df['pickup_latitude'], df['pickup_longitude'],\n",
    "                                df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    df['manhattan'] = manhattan(df['pickup_latitude'], df['pickup_longitude'],\n",
    "                                df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    df['bearing'] = bearing(df['pickup_latitude'], df['pickup_longitude'],\n",
    "                            df['dropoff_latitude'], df['dropoff_longitude'])\n",
    "    df['speed'] = df['haversine'] * 1000 / (df['trip_duration'] + 1e-8)  # m/s, handle zero duration\n",
    "    # Categorical\n",
    "    df['store_and_fwd_flag'] = (df['store_and_fwd_flag'] == 'Y').astype(int)\n",
    "    # Cluster (fit on train only)\n",
    "    coords = np.vstack([df['pickup_latitude'], df['pickup_longitude'],\n",
    "                        df['dropoff_latitude'], df['dropoff_longitude']]).T\n",
    "    if fit_kmeans is not None:\n",
    "        km = fit_kmeans\n",
    "    else:\n",
    "        km = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)\n",
    "        km.fit(coords)\n",
    "    clusters = km.predict(coords)\n",
    "    df['pickup_dropoff_cluster'] = clusters\n",
    "    return df\n",
    "\n",
    "print(\"Feature engineering train...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"c:\\Users\\SamGostarCO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SamGostarCO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\SamGostarCO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\SamGostarCO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42, n_init=10)\n",
    "# Use train to fit clusters, apply to both sets\n",
    "_ = build_features(train, fit_kmeans=None)  # fit separate for cluster fitting\n",
    "kmeans.fit(np.vstack([\n",
    "    train[['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude']].values,\n",
    "    test[['pickup_latitude','pickup_longitude','dropoff_latitude','dropoff_longitude']].values\n",
    "]))\n",
    "train = build_features(train, fit_kmeans=kmeans)\n",
    "test = build_features(test, fit_kmeans=kmeans)\n",
    "\n",
    "# --- define features\n",
    "FEATURES = [\n",
    "    # Numeric/date\n",
    "    'pickup_year', 'pickup_month', 'pickup_day', 'pickup_hour', 'pickup_minute',\n",
    "    'pickup_weekday', 'is_weekend', 'pickup_hour_sin', 'pickup_hour_cos',\n",
    "    'pickup_minute_sin','pickup_minute_cos','rushhour','daypart','is_night','is_holiday',\n",
    "    'haversine', 'manhattan', 'bearing', 'trip_duration','speed',\n",
    "    # Categorical\n",
    "    'passenger_count', 'store_and_fwd_flag', 'pickup_dropoff_cluster'\n",
    "]\n",
    "\n",
    "X = train[FEATURES].copy()\n",
    "y = train['total_price'].copy()\n",
    "X_test = test[FEATURES].copy()\n",
    "test_ids = test['id'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- categorical encoding\n",
    "cat_cols = ['passenger_count', 'store_and_fwd_flag', 'pickup_dropoff_cluster']\n",
    "for data in [X, X_test]:\n",
    "    for col in cat_cols:\n",
    "        data[col] = data[col].astype('category')\n",
    "\n",
    "# --- time-based cross-validation\n",
    "pickup_dt = pd.to_datetime(train['pickup_datetime'])\n",
    "months = pickup_dt.dt.to_period('M')\n",
    "month_labels = months.astype(str)\n",
    "month_uniques = np.unique(month_labels)\n",
    "# Use months for splits (leave last period for val each fold)\n",
    "folds = []\n",
    "for i in range(N_SPLITS):\n",
    "    val_month = month_uniques[i % len(month_uniques)]\n",
    "    train_idx = np.where(month_labels != val_month)[0]\n",
    "    val_idx = np.where(month_labels == val_month)[0]\n",
    "    folds.append((train_idx, val_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- model wrapper\n",
    "def get_model(model_name, seed):\n",
    "    if model_name == 'ridge':\n",
    "        return Ridge(alpha=0.5, random_state=seed)\n",
    "    elif model_name == 'linear':\n",
    "        return LinearRegression()\n",
    "    elif model_name == 'lgbm':\n",
    "        params = dict(\n",
    "            n_estimators=500,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=seed,\n",
    "            subsample=0.85,\n",
    "            colsample_bytree=0.95,\n",
    "            n_jobs=-1,\n",
    "            objective='regression',\n",
    "            device_type=\"gpu\" if USE_GPU else \"cpu\"\n",
    "        )\n",
    "        return lgb.LGBMRegressor(**params)\n",
    "    elif model_name == 'catboost':\n",
    "        params = dict(\n",
    "            iterations=500,\n",
    "            depth=6,\n",
    "            learning_rate=0.08,\n",
    "            random_seed=seed,\n",
    "            loss_function='RMSE',\n",
    "            cat_features=[FEATURES.index(c) for c in cat_cols],\n",
    "            task_type='GPU' if USE_GPU else 'CPU',\n",
    "            verbose=0\n",
    "        )\n",
    "        return cb.CatBoostRegressor(**params)\n",
    "    elif model_name == 'xgboost':\n",
    "        params = dict(\n",
    "            n_estimators=500,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=seed,\n",
    "            subsample=0.85,\n",
    "            colsample_bytree=0.95,\n",
    "            n_jobs=-1,\n",
    "            objective='reg:squarederror',\n",
    "            tree_method='gpu_hist' if USE_GPU else 'hist'\n",
    "        )\n",
    "        return xgb.XGBRegressor(**params)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- scoring\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def comp_score(rmse_val, y_true):\n",
    "    std = np.std(y_true)\n",
    "    return 100.0 * np.exp(-rmse_val / std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "آموزش مدل\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: justify;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    حال که داده را پاکسازی کرده و احتمالا ویژگی‌هایی را به آن افزوده یا از آن حذف کرده‌اید، وقت آن است که مدلی آموزش دهید که بتواند متغیر هدف این مسئله را پیش‌بینی کند. در این قسمت استفاده از هر نوع مدل مربوط به یادگیری ماشین کلاسیک مجاز می‌باشد.\n",
    "    در این قسمت مدل مورد نظر باید طوری آموزش داده شود که قادر به پیش‌بینی مقادیر ستون <code>rent</code> باشد.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv(X, y, X_test, model_name, n_folds, seeds):\n",
    "    oof = np.zeros(len(X))\n",
    "    test_preds = np.zeros((len(X_test), len(seeds)))\n",
    "    models = []\n",
    "    val_idxs = []\n",
    "\n",
    "    for si, seed in enumerate(seeds):\n",
    "        fold_oof = np.zeros(len(X))\n",
    "        fold_test_preds = []\n",
    "        models_seed = []\n",
    "\n",
    "        for fold, (tr_idx, val_idx) in enumerate(folds):\n",
    "            X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "            y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "            y_tr_log = np.log1p(y_tr)\n",
    "            model = get_model(model_name, seed)\n",
    "\n",
    "            if model_name in ['ridge', 'linear']:\n",
    "                scale = StandardScaler()\n",
    "                X_trs = scale.fit_transform(X_tr)\n",
    "                X_vals = scale.transform(X_val)\n",
    "                X_tests = scale.transform(X_test)\n",
    "                model.fit(X_trs, y_tr_log)\n",
    "                y_val_pred = np.expm1(model.predict(X_vals))\n",
    "                y_test_pred = np.expm1(model.predict(X_tests))\n",
    "\n",
    "            elif model_name == 'lgbm':\n",
    "                model.fit(\n",
    "                    X_tr, y_tr_log,\n",
    "                    eval_set=[(X_val, np.log1p(y_val))],\n",
    "                    callbacks=[lgb.early_stopping(30), lgb.log_evaluation(0)]\n",
    "                )\n",
    "                y_val_pred = np.expm1(model.predict(X_val))\n",
    "                y_test_pred = np.expm1(model.predict(X_test))\n",
    "\n",
    "            elif model_name == 'catboost':\n",
    "                model.fit(\n",
    "                    X_tr, y_tr_log,\n",
    "                    eval_set=(X_val, np.log1p(y_val)),\n",
    "                    early_stopping_rounds=30,\n",
    "                    verbose=0\n",
    "                )\n",
    "                y_val_pred = np.expm1(model.predict(X_val))\n",
    "                y_test_pred = np.expm1(model.predict(X_test))\n",
    "\n",
    "            elif model_name == 'xgboost':\n",
    "                model.fit(\n",
    "                    X_tr, y_tr_log,\n",
    "                    eval_set=[(X_val, np.log1p(y_val))],\n",
    "                    early_stopping_rounds=30,\n",
    "                    verbose=0\n",
    "                )\n",
    "                y_val_pred = np.expm1(model.predict(X_val))\n",
    "                y_test_pred = np.expm1(model.predict(X_test))\n",
    "\n",
    "            else:\n",
    "                # generic fallback (may not use early stopping)\n",
    "                model.fit(X_tr, y_tr_log)\n",
    "                y_val_pred = np.expm1(model.predict(X_val))\n",
    "                y_test_pred = np.expm1(model.predict(X_test))\n",
    "\n",
    "            y_val_pred = np.clip(y_val_pred, 0, None)\n",
    "            fold_oof[val_idx] = y_val_pred\n",
    "            fold_test_preds.append(y_test_pred)\n",
    "            models_seed.append(model)\n",
    "            val_idxs.extend(val_idx)\n",
    "\n",
    "        models.append(models_seed)\n",
    "        test_preds[:, si] = np.mean(fold_test_preds, axis=0)\n",
    "        oof += fold_oof / len(seeds)\n",
    "\n",
    "    test_preds_mean = test_preds.mean(axis=1)\n",
    "    return oof, test_preds_mean, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ridge model...\n",
      "Ridge: RMSE=323152006184718440777035619523151003648.0000 Score=0.00\n"
     ]
    }
   ],
   "source": [
    "# --- Baseline: Ridge regression\n",
    "print(\"Training Ridge model...\")\n",
    "ridge_oof, ridge_test, ridge_models = train_cv(X, y, X_test, 'ridge', N_SPLITS, SEEDS)\n",
    "ridge_rmse = rmse(y, ridge_oof)\n",
    "ridge_score = comp_score(ridge_rmse, y)\n",
    "print(f\"Ridge: RMSE={ridge_rmse:.4f} Score={ridge_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM model...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1587\n",
      "[LightGBM] [Info] Number of data points in the train set: 1064568, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.958103\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid_0's l2: 0.0107783\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1587\n",
      "[LightGBM] [Info] Number of data points in the train set: 1047592, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.957013\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[154]\tvalid_0's l2: 0.0126544\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1587\n",
      "[LightGBM] [Info] Number of data points in the train set: 1031127, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.964271\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[107]\tvalid_0's l2: 0.00990374\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031183 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1587\n",
      "[LightGBM] [Info] Number of data points in the train set: 1037128, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.960933\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid_0's l2: 0.0091476\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1586\n",
      "[LightGBM] [Info] Number of data points in the train set: 1041735, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.958170\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[147]\tvalid_0's l2: 0.00807512\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1588\n",
      "[LightGBM] [Info] Number of data points in the train set: 1064568, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.958103\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's l2: 0.0107955\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1586\n",
      "[LightGBM] [Info] Number of data points in the train set: 1047592, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.957013\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid_0's l2: 0.0128201\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026541 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1587\n",
      "[LightGBM] [Info] Number of data points in the train set: 1031127, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.964271\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's l2: 0.0104552\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042696 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1587\n",
      "[LightGBM] [Info] Number of data points in the train set: 1037128, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.960933\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid_0's l2: 0.00915035\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1587\n",
      "[LightGBM] [Info] Number of data points in the train set: 1041735, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.958170\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid_0's l2: 0.00815383\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.044908 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1586\n",
      "[LightGBM] [Info] Number of data points in the train set: 1064568, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.958103\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid_0's l2: 0.0108513\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1587\n",
      "[LightGBM] [Info] Number of data points in the train set: 1047592, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.957013\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid_0's l2: 0.012746\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1587\n",
      "[LightGBM] [Info] Number of data points in the train set: 1031127, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.964271\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[94]\tvalid_0's l2: 0.0102589\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1586\n",
      "[LightGBM] [Info] Number of data points in the train set: 1037128, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.960933\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid_0's l2: 0.00920757\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1587\n",
      "[LightGBM] [Info] Number of data points in the train set: 1041735, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 2.958170\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid_0's l2: 0.00810403\n",
      "LGBM: RMSE=39.5932 Score=43.91\n"
     ]
    }
   ],
   "source": [
    "# --- LGBM\n",
    "print(\"Training LightGBM model...\")\n",
    "lgbm_oof, lgbm_test, lgbm_models = train_cv(X, y, X_test, 'lgbm', N_SPLITS, SEEDS)\n",
    "lgbm_rmse = rmse(y, lgbm_oof)\n",
    "lgbm_score = comp_score(lgbm_rmse, y)\n",
    "print(f\"LGBM: RMSE={lgbm_rmse:.4f} Score={lgbm_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CatBoost model...\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "bad allocation",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCatBoostError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- CatBoost\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining CatBoost model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m cat_oof, cat_test, cat_models = \u001b[43mtrain_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcatboost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_SPLITS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSEEDS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m cat_rmse = rmse(y, cat_oof)\n\u001b[32m      5\u001b[39m cat_score = comp_score(cat_rmse, y)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mtrain_cv\u001b[39m\u001b[34m(X, y, X_test, model_name, n_folds, seeds)\u001b[39m\n\u001b[32m     34\u001b[39m     y_test_pred = np.expm1(model.predict(X_test))\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_name == \u001b[33m'\u001b[39m\u001b[33mcatboost\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog1p\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     y_val_pred = np.expm1(model.predict(X_val))\n\u001b[32m     44\u001b[39m     y_test_pred = np.expm1(model.predict(X_test))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SamGostarCO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py:5873\u001b[39m, in \u001b[36mCatBoostRegressor.fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   5871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[32m   5872\u001b[39m     CatBoostRegressor._check_is_compatible_loss(params[\u001b[33m'\u001b[39m\u001b[33mloss_function\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m5873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5874\u001b[39m \u001b[43m                 \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5875\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5876\u001b[39m \u001b[43m                 \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SamGostarCO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py:2410\u001b[39m, in \u001b[36mCatBoost._fit\u001b[39m\u001b[34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[39m\n\u001b[32m   2407\u001b[39m allow_clear_pool = train_params[\u001b[33m\"\u001b[39m\u001b[33mallow_clear_pool\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   2409\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[33m'\u001b[39m\u001b[33mTraining plots\u001b[39m\u001b[33m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m.get_params())]):\n\u001b[32m-> \u001b[39m\u001b[32m2410\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2411\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2412\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meval_sets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2413\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2414\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2415\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minit_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   2416\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2418\u001b[39m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[32m   2419\u001b[39m loss = \u001b[38;5;28mself\u001b[39m._object._get_loss_function_name()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SamGostarCO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py:1790\u001b[39m, in \u001b[36m_CatBoostBase._train\u001b[39m\u001b[34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_trained_model_attributes()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5023\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_catboost.pyx:5072\u001b[39m, in \u001b[36m_catboost._CatBoost._train\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mCatBoostError\u001b[39m: bad allocation"
     ]
    }
   ],
   "source": [
    "# --- CatBoost\n",
    "print(\"Training CatBoost model...\")\n",
    "cat_oof, cat_test, cat_models = train_cv(X, y, X_test, 'catboost', N_SPLITS, SEEDS)\n",
    "cat_rmse = rmse(y, cat_oof)\n",
    "cat_score = comp_score(cat_rmse, y)\n",
    "print(f\"CatBoost: RMSE={cat_rmse:.4f} Score={cat_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking meta-learner Ridge...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cat_oof' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Stacking\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStacking meta-learner Ridge...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m oof_stack = np.vstack([ridge_oof, lgbm_oof, \u001b[43mcat_oof\u001b[49m]).T\n\u001b[32m      4\u001b[39m test_stack = np.vstack([ridge_test, lgbm_test, cat_test]).T\n\u001b[32m      5\u001b[39m meta = Ridge(alpha=\u001b[32m0.3\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'cat_oof' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Stacking\n",
    "print(\"Stacking meta-learner Ridge...\")\n",
    "oof_stack = np.vstack([ridge_oof, lgbm_oof, cat_oof]).T\n",
    "test_stack = np.vstack([ridge_test, lgbm_test, cat_test]).T\n",
    "meta = Ridge(alpha=0.3)\n",
    "meta.fit(oof_stack, y)\n",
    "stack_oof = meta.predict(oof_stack)\n",
    "stack_test = meta.predict(test_stack)\n",
    "stack_oof = np.clip(stack_oof, 0, None)\n",
    "stack_test = np.clip(stack_test, 0, None)\n",
    "stack_rmse = rmse(y, stack_oof)\n",
    "stack_score = comp_score(stack_rmse, y)\n",
    "print(f\"Stacking: RMSE={stack_rmse:.4f} Score={stack_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "معیار ارزیابی\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    معیاری که برای ارزیابی عملکرد مدل انتخاب کرده‌ایم، <code>r2_score</code> نام دارد.\n",
    "    <br>\n",
    "    این معیار، سنجه ارزیابی کیفیت مدل شماست. به عبارت بهتر در سامانه داوری هم از همین معیار برای نمره‌دهی استفاده شده است.\n",
    "    <br>\n",
    "    پیشنهاد می‌شود با توجه به این معیار، عملکرد مدل خود را بر روی مجموعه‌ی آموزش یا اعتبارسنجی ارزیابی کنید.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font color=\"red\"><b color='red'>توجه:</b></font>\n",
    "<font face=\"vazir\" size=3>\n",
    "    برای دریافت نمره از این سوال لازم است تا دقت مدل شما از آستانه‌ی ۰.۵ بیشتر باشد.\n",
    "    در صورتی که دقت مدل شما از ۰.۵ کمتر باشد نمره شما \n",
    "    <b>صفر</b>\n",
    "    خواهد شد و در غیر این صورت با فرمول زیر محاسبه می‌شود:\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#--- LEADERBOARD ---#\n",
      " Ridge    : RMSE=323152006184718440777035619523151003648.0000 Score=0.00\n",
      " LightGBM : RMSE=39.5932 Score=43.91\n",
      "Submission written to: result.zip\n"
     ]
    }
   ],
   "source": [
    "# --- Final prediction: weighted ensemble by CV (highest gets more weight)\n",
    "all_preds = np.vstack([\n",
    "    ridge_test,\n",
    "    lgbm_test,\n",
    "    #cat_test,\n",
    "    #stack_test\n",
    "])\n",
    "scores = np.array([ridge_score, lgbm_score])\n",
    "weights = scores / scores.sum()\n",
    "final_pred = np.average(all_preds, axis=0, weights=weights)\n",
    "final_pred = np.clip(final_pred, 0, None)\n",
    "final_pred = np.round(final_pred, 2)\n",
    "\n",
    "# --- Submission\n",
    "sub = pd.DataFrame({'id': test_ids, 'total_price': final_pred})\n",
    "sub.to_csv(SUB_PATH, index=False)\n",
    "with zipfile.ZipFile(RESULT_ZIP, 'w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(SUB_PATH)\n",
    "\n",
    "# --- Save models\n",
    "joblib.dump(ridge_models, f\"{MODEL_DIR}/ridge_models.joblib\")\n",
    "joblib.dump(lgbm_models, f\"{MODEL_DIR}/lgbm_models.joblib\")\n",
    "# joblib.dump(cat_models, f\"{MODEL_DIR}/catboost_models.joblib\")\n",
    "# joblib.dump(meta, f\"{MODEL_DIR}/stacker_ridge.joblib\")\n",
    "\n",
    "# --- Print summary\n",
    "print(\"#--- LEADERBOARD ---#\")\n",
    "print(f\" Ridge    : RMSE={ridge_rmse:.4f} Score={ridge_score:.2f}\")\n",
    "print(f\" LightGBM : RMSE={lgbm_rmse:.4f} Score={lgbm_score:.2f}\")\n",
    "# print(f\" CatBoost : RMSE={cat_rmse:.4f} Score={cat_score:.2f}\")\n",
    "# print(f\" Stacking : RMSE={stack_rmse:.4f} Score={stack_score:.2f}\")\n",
    "print(\"Submission written to:\", RESULT_ZIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    " پیش‌بینی برای داده تست و خروجی\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl;text-align: right;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    پیش‌بینی مدل خود بر روی داده‌های آزمایش (<code>test.csv</code>) را در قالب یک <code>dataframe</code> در متغیری با نام <code>submission</code>ذخیره کنید.  این <code>dataframe</code> باید دارای یک ستون با نام <code>total_price</code> باشد که ردیف <code>i</code>ام آن، پیش‌بینی شما برای سطر <code>i</code>ام مجموعه‌داده‌ی آزمون باشد.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<center>\n",
    "<div dir=rtl style=\"direction: rtl;line-height:200%;font-family:vazir;font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    \n",
    "|ستون|توضیحات|\n",
    "|------|---|\n",
    "|total_price|قیمت نهایی کل سفر به دلار|\n",
    "    \n",
    "</font>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "\n",
    "submission = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 dir=rtl align=right style=\"line-height:200%;font-family:vazir;color:#0099cc\">\n",
    "<font face=\"vazir\" color=\"#0099cc\">\n",
    "<b>سلول جواب‌ساز</b>\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=rtl style=\"direction: rtl; text-align: justify; line-height:200%; font-family:vazir; font-size:medium\">\n",
    "<font face=\"vazir\" size=3>\n",
    "    برای ساخته‌شدن فایل <code>result.zip</code> سلول زیر را اجرا کنید. توجه داشته باشید که پیش از اجرای سلول زیر تغییرات اعمال شده در نت‌بوک را ذخیره کرده باشید (<code>ctrl+s</code>) در غیر این صورت، در پایان مسابقه نمره شما به صفر تغییر خواهد کرد.\n",
    "    <br>\n",
    "    همچنین اگر از کولب برای اجرای این فایل نوت‌بوک استفاده می‌کنید، قبل از ارسال فایل <code>result.zip</code>، آخرین نسخه‌ی نوت‌بوک خود را دانلود کرده و داخل فایل ارسالی قرار دهید.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: suppose your predictions are in a variable called final_pred\n",
    "# and your test set IDs are in test_ids\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'total_price': final_pred\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Paths:\n",
      "['TeaOverflowError.ipynb', 'submission.csv']\n"
     ]
    }
   ],
   "source": [
    "    submission.to_csv('submission.csv', index=False)\n",
    "    file_names = ['TeaOverflowError.ipynb', 'submission.csv']\n",
    "    compress(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Paths:\n",
      "['TeaOverflowError.ipynb', 'submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Export notebook if not exists (only in Jupyter, safe to skip if on Colab or already have the file)\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'TeaOverflowError.ipynb')):\n",
    "    try:\n",
    "        get_ipython().run_line_magic('notebook', '-e TeaOverflowError.ipynb')\n",
    "    except Exception as e:\n",
    "        print(\"Notebook export magic may not be supported in this environment.\", e)\n",
    "\n",
    "def compress(file_names):\n",
    "    print(\"File Paths:\")\n",
    "    print(file_names)\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
    "        for file_name in file_names:\n",
    "            zf.write('./' + file_name, file_name, compress_type=compression)\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "file_names = ['TeaOverflowError.ipynb', 'submission.csv']\n",
    "compress(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'submission' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m file_names:\n\u001b[32m     14\u001b[39m             zf.write(\u001b[33m'\u001b[39m\u001b[33m./\u001b[39m\u001b[33m'\u001b[39m + file_name, file_name, compress_type=compression)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43msubmission\u001b[49m.to_csv(\u001b[33m'\u001b[39m\u001b[33msubmission.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     17\u001b[39m file_names = [\u001b[33m'\u001b[39m\u001b[33mTeaOverflowError.ipynb\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msubmission.csv\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     18\u001b[39m compress(file_names)\n",
      "\u001b[31mNameError\u001b[39m: name 'submission' is not defined"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "if not os.path.exists(os.path.join(os.getcwd(), 'TeaOverflowError.ipynb')):\n",
    "    %notebook -e TeaOverflowError.ipynb\n",
    "\n",
    "def compress(file_names):\n",
    "    print(\"File Paths:\")\n",
    "    print(file_names)\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
    "        for file_name in file_names:\n",
    "            zf.write('./' + file_name, file_name, compress_type=compression)\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "file_names = ['TeaOverflowError.ipynb', 'submission.csv']\n",
    "compress(file_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
